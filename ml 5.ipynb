{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: What are the key tasks that machine learning entails? What does data pre-processing imply?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are Five core tasks in the common ML workflow:\n",
    "\n",
    "1.Get Data: The first step in the Machine Learning process is getting data.\n",
    "\n",
    "2.Cleaning, Preparing & Manipulating Data: Real-world data often has unorganized, missing, or noisy elements.\n",
    "\n",
    "3.Train Model: This step is where the magic happens!\n",
    "\n",
    "4.Testing Model.\n",
    "\n",
    "5.Improving model.\n",
    "\n",
    "6.Data preprocessing involves transforming raw data to well-formed data sets so that data mining analytics can be applied.\n",
    "\n",
    "7.Preprocessing involves both data validation and data imputation\n",
    "\n",
    "8.The Goal of Data Validation is to assess whether the data in question is both complete and accurate.\n",
    "\n",
    "9.The Goal of Data Imputation is to correct errors and input missing values, Either Manually or Automatically through business process automation (BPA) programming."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2: Describe quantitative and qualitative data in depth. Make a distinction between the two."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantitative and qualitative data are two fundamental types of data used in research and analysis. They differ in terms of their nature, methodology, and the type of information they provide. Here's a detailed description of both types:\n",
    "\n",
    "Quantitative Data:\n",
    "Quantitative data involves measurements, numerical values, or data that can be expressed in numerical form. It deals with objective and measurable information, usually obtained through structured methods. This type of data focuses on quantities, frequencies, and statistical analysis. It is commonly used in fields such as statistics, economics, psychology, and natural sciences. Here are some key characteristics of quantitative data:\n",
    "\n",
    "1. Measurement: Quantitative data relies on standardized measurements or numerical values. Examples include height, weight, temperature, age, income, and test scores.\n",
    "\n",
    "2. Objectivity: It aims to be objective and free from personal biases or interpretations. The data is collected using systematic and predefined procedures to ensure consistency and reliability.\n",
    "\n",
    "3. Large Sample Sizes: Quantitative data often requires large sample sizes to generate statistically significant results and make generalizations about a population.\n",
    "\n",
    "4. Statistical Analysis: Quantitative data is suitable for statistical analysis, including descriptive statistics (mean, median, mode), inferential statistics (hypothesis testing, regression analysis), and correlation analysis.\n",
    "\n",
    "5. Data Collection Methods: Common methods for collecting quantitative data include surveys, experiments, structured observations, and analyzing existing datasets.\n",
    "\n",
    "6. Quantifiable Relationships: It allows for establishing relationships between variables through statistical techniques, such as regression analysis or correlation coefficients.\n",
    "\n",
    "Qualitative Data:\n",
    "Qualitative data involves non-numerical, descriptive information that provides insights into the qualities, characteristics, and meanings of phenomena. It focuses on subjective experiences, opinions, behaviors, and social interactions. Qualitative data is commonly used in fields such as anthropology, sociology, psychology, and humanities. Here are some key characteristics of qualitative data:\n",
    "\n",
    "1. Description and Interpretation: Qualitative data focuses on describing and interpreting complex phenomena, often in-depth and rich detail. It aims to capture the context, nuances, and subjective experiences of individuals or groups.\n",
    "\n",
    "2. Subjectivity: Unlike quantitative data, qualitative data acknowledges subjectivity and recognizes that multiple interpretations and perspectives can exist. Researchers play an active role and may use their own judgment in analyzing and interpreting the data.\n",
    "\n",
    "3. Small Sample Sizes: Qualitative data often involves smaller sample sizes, allowing for a deeper understanding of specific cases or contexts rather than making generalizations about a larger population.\n",
    "\n",
    "4. Data Collection Methods: Common methods for collecting qualitative data include interviews, focus groups, participant observation, case studies, and analysis of documents or artifacts.\n",
    "\n",
    "5. Themes and Patterns: Qualitative data analysis focuses on identifying themes, patterns, and relationships within the data. Techniques like coding and thematic analysis are used to extract key insights and generate theories or hypotheses.\n",
    "\n",
    "6. Contextual Understanding: Qualitative data provides a holistic understanding of social and cultural contexts, allowing researchers to explore the \"why\" and \"how\" behind phenomena.\n",
    "\n",
    "In summary, quantitative data deals with numerical measurements, objective information, and statistical analysis, while qualitative data focuses on descriptive, subjective, and contextual information, providing in-depth insights and interpretations. Both types of data play important roles in research, and their combination can enhance the comprehensiveness and validity of findings."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3: Create a basic data collection that includes some sample records. Have at least one attribute from\n",
    "each of the machine learning data types.\n",
    "\n",
    "The following is a basic data collection that includes some sample records.\n",
    "\n",
    "1.Determine What Information You Want to Collect: The first thing you need to do is choose what details you want to collect. You’ll need to decide what topics the information will cover, who you want to collect it from and how much data you need. Your goals — what you hope to accomplish using your data — will determine your answers to these questions. As an example, you may decide to collect data about which type of articles are most popular on your website among visitors who are between the ages of 18 and 34. You might also choose to gather information about the average age of all of the customers who bought a product from your company within the last month.\n",
    "\n",
    "2.Set a Timeframe for Data Collection: Next, you can start formulating your plan for how you’ll collect your data. In the early stages of your planning process, you should establish a timeframe for your data collection. You may want to gather some types of data continuously. When it comes to transactional data and website visitor data, for example, you may want to set up a method for tracking that data over the long term. If you’re tracking data for a specific campaign, however, you’ll track it over a defined period. In these instances, you’ll have a schedule for when you’ll start and end your data collection.\n",
    "\n",
    "3.Determine Your Data Collection Method: At this step, you will choose the data collection method that will make up the core of your data-gathering strategy. To select the right collection method, you’ll need to consider the type of information you want to collect, the timeframe over which you’ll obtain it and the other aspects you determined.\n",
    "\n",
    "4.Collect the Data: Once you have finalized your plan, you can implement your data collection strategy and start collecting data. You can store and organize your data in your DMP. Be sure to stick to your plan and check on its progress regularly. It may be useful to create a schedule for when you will check in with how your data collection is proceeding, especially if you are collecting data continuously. You may want to make updates to your plan as conditions change and you get new information.\n",
    "\n",
    "5.Analyze the Data and Implement Your Findings: Once you’ve collected all of your data, it’s time to analyze it and organize your findings. The analysis phase is crucial because it turns raw data into valuable insights that you can use to enhance your marketing strategies, products and business decisions. You can use the analytics tools built into our DMP to help with this step. Once you’ve uncovered the patterns and insights in your data, you can implement the findings to improve your business."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4: What are the various causes of machine learning data issues? What are the ramifications?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning data issues can arise from various sources and have significant ramifications on the performance and reliability of the models. Here are some common causes of data issues in machine learning:\n",
    "\n",
    "1. Insufficient or Biased Data: Lack of sufficient and representative data can lead to poor model performance. If the dataset is biased or does not adequately cover the target population, the model may produce biased or inaccurate predictions.\n",
    "\n",
    "2. Data Quality and Inconsistency: Inaccurate, incomplete, or inconsistent data can introduce errors and noise into the training process. Outliers, missing values, or incorrect labels can negatively impact model performance.\n",
    "\n",
    "3. Imbalanced Data: When the distribution of classes in a dataset is highly imbalanced, with one class significantly outnumbering others, the model may have difficulty learning patterns from the minority class. This can lead to biased predictions and reduced accuracy.\n",
    "\n",
    "4. Noisy Data: Noisy data contains irrelevant or misleading information that can confuse the learning algorithm. It may include errors, outliers, or irrelevant features that hinder the model's ability to generalize accurately.\n",
    "\n",
    "5. Overfitting: Overfitting occurs when a model learns the specific patterns and noise present in the training data too well, resulting in poor generalization to new, unseen data. This issue arises when the model becomes overly complex or when the dataset is small.\n",
    "\n",
    "6. Data Leakage: Data leakage refers to situations where information from the test set or future data inadvertently influences the model during training. It can lead to overly optimistic performance estimates and models that fail to generalize well.\n",
    "\n",
    "7. Feature Selection and Engineering: Choosing relevant features and engineering them appropriately is crucial for model performance. If important features are omitted or irrelevant features are included, it can impact the model's ability to learn meaningful patterns.\n",
    "\n",
    "The ramifications of these data issues can be significant:\n",
    "\n",
    "1. Reduced Accuracy: Data issues can lead to reduced model accuracy, making predictions less reliable and less valuable in real-world applications.\n",
    "\n",
    "2. Biased Predictions: Biased or skewed data can result in biased predictions, perpetuating or amplifying existing inequalities or prejudices present in the data.\n",
    "\n",
    "3. Increased Error Rates: Noisy, inconsistent, or inadequate data can increase the error rates of the model, affecting its ability to make correct predictions or decisions.\n",
    "\n",
    "4. Poor Generalization: Issues like overfitting or data leakage can cause models to perform well on the training data but fail to generalize to new, unseen data. This limits their usefulness in real-world scenarios.\n",
    "\n",
    "5. Misinterpretation of Results: If data issues are not properly addressed, the interpretation of model results and insights derived from them may be misleading or incorrect.\n",
    "\n",
    "To mitigate these issues, it is crucial to thoroughly preprocess and clean the data, address bias and imbalance, validate the quality of the dataset, and employ appropriate techniques for feature engineering, regularization, and model evaluation. Regular monitoring and updating of the data pipeline and models can help ensure ongoing reliability and performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5: Demonstrate various approaches to categorical data exploration with appropriate examples.\n",
    ": Various approaches to categorical data exploration are:\n",
    "\n",
    "1.Unique value count: One of the first things which can be useful during data exploration is to see how many unique values are there in categorical columns.\n",
    "\n",
    "2.Frequency Count: Frequency count is finding how frequent individual values occur in column.\n",
    "\n",
    "3.Variance: Variance gives a good indication how the values are spread.\n",
    "\n",
    "4.Pareto Analysis: Pareto analysis is a creative way of focusing on what is important. Pareto 80–20 rule can be effectively used in data exploration.\n",
    "\n",
    "5.Histogram: Histogram are one of the data scientists favourite data exploration techniques. It gives information on the range of values in which most of the values fall. It also gives information on whether there is any skew in data.\n",
    "\n",
    "6.Correlation Heat-map between all numeric columns: The term correlation refers to a mutual relationship or association between two things.\n",
    "\n",
    "7.Pearson Correlation and Trend between two numeric columns: Once you have visualised correlation heat-map , the next step is to see the correlation trend between two specific numeric columns.\n",
    "\n",
    "8.Outlier overview: Finding something unusual in data is called Outlier detection (also known as anomaly detection). These outliers represent something unusual, rare , anomaly or something exceptional."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6: How would the learning activity be affected if certain variables have missing values? Having said\n",
    "that, what can be done about it?\n",
    "\n",
    "Even in a Well-Designed & Controlled study, Missing data occurs in almost all research. Missing data can reduce the statistical power of a study and can produce biased estimates, leading to invalid conclusions.\n",
    "\n",
    "1.Real-world data collection has its own set of problems, It is often very messy which includes missing data, presence of outliers, unstructured manner, etc.\n",
    "\n",
    "2.Before looking for any insights from the data, we have to first perform preprocessing tasks which then only allow us to use that data for further observation and train our machine learning model.\n",
    "\n",
    "3.Missing value in a dataset is a very common phenomenon in the reality.\n",
    "\n",
    "4.Missing value correction is required to reduce bias and to produce powerful suitable models.\n",
    "\n",
    "5.Most of the algorithms can’t handle missing data, thus you need to act in some way to simply not let your code crash. So, let’s begin with the methods to solve the problem.\n",
    "\n",
    "6.Methods for dealing with missing values. The popular methods which are used by the machine learning community to handle the missing value for categorical variables in the dataset are as follows: Delete the observations: If there is a large number of observations in the dataset, where all the classes to be predicted are sufficiently represented in the training data, then try deleting the missing value observations, which would not bring significant change in your feed to your model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7: Describe the various methods for dealing with missing data values in depth.\n",
    "The Various Methods for dealing with missing data values are:\n",
    "\n",
    "1.Delete the observations: If there is a large number of observations in the dataset, where all the classes to be predicted are sufficiently represented in the training data, then try deleting the missing value observations, which would not bring significant change in your feed to your model. For Example Implement this method in a given dataset, we can delete the entire row which contains missing values.\n",
    "\n",
    "2.Replace missing values with the most frequent value: You can always impute them based on Mode in the case of categorical variables, just make sure you don’t have highly skewed class distributions.\n",
    "\n",
    "3.Develop a model to predict missing values: One smart way of doing this could be training a classifier over your columns with missing values as a dependent variable against other features of your data set and trying to impute based on the newly trained classifier."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8: What are the various data pre-processing techniques? Explain dimensionality reduction and\n",
    "function selection in a few words."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessing techniques are applied to prepare raw data for machine learning algorithms. They involve transforming and cleaning the data to improve its quality, remove noise, handle missing values, and make it suitable for analysis. Some common data preprocessing techniques include:\n",
    "\n",
    "1. Data Cleaning: This involves handling missing data by imputation or removal, correcting inconsistencies, dealing with outliers, and ensuring data integrity.\n",
    "\n",
    "2. Data Transformation: This technique involves transforming the data to adhere to certain assumptions of the model, such as normalization (scaling the data to a specific range) or standardization (transforming data to have zero mean and unit variance).\n",
    "\n",
    "3. Feature Scaling: Scaling the features ensures that they have a similar range, preventing some features from dominating others during model training. Common scaling techniques include min-max scaling and z-score standardization.\n",
    "\n",
    "4. Encoding Categorical Variables: Categorical variables need to be converted into numerical form for machine learning algorithms. Techniques like one-hot encoding or label encoding are used for this purpose.\n",
    "\n",
    "5. Handling Imbalanced Data: Imbalanced datasets, where one class is significantly more prevalent than others, require techniques like oversampling the minority class, undersampling the majority class, or using synthetic data generation methods.\n",
    "\n",
    "6. Dimensionality Reduction: Dimensionality reduction techniques reduce the number of features in the dataset while preserving relevant information. This helps in reducing computational complexity, mitigating the curse of dimensionality, and extracting the most important features. One popular method is Principal Component Analysis (PCA).\n",
    "\n",
    "7. Feature Selection: Feature selection involves identifying the most relevant and informative features from the dataset. It helps in reducing noise, improving model interpretability, and enhancing generalization. Techniques like filter methods, wrapper methods, and embedded methods are commonly used for feature selection.\n",
    "\n",
    "Now, let's briefly explain dimensionality reduction and feature selection:\n",
    "\n",
    "1. Dimensionality Reduction: Dimensionality reduction refers to the process of reducing the number of features or variables in a dataset while retaining the most important information. It is useful when dealing with high-dimensional datasets where the number of features is large. The goal is to transform the data into a lower-dimensional space while preserving as much meaningful variation as possible. Dimensionality reduction techniques, such as PCA, aim to identify the principal components or directions that capture the maximum variance in the data. By projecting the data onto a lower-dimensional subspace defined by these principal components, redundant or less informative features are eliminated or combined, reducing the computational burden and potentially improving model performance.\n",
    "\n",
    "2. Feature Selection: Feature selection involves selecting a subset of relevant features from the original dataset. It aims to identify the most informative features that have the strongest relationship with the target variable, while discarding irrelevant or redundant features. Feature selection can improve model efficiency, reduce overfitting, enhance interpretability, and mitigate the curse of dimensionality. There are various feature selection techniques, including filter methods (using statistical measures to rank features), wrapper methods (employing a specific model's performance as a criterion for feature selection), and embedded methods (incorporating feature selection within the model training process).\n",
    "\n",
    "Both dimensionality reduction and feature selection are valuable techniques in data preprocessing as they help in simplifying the dataset, reducing noise, improving computational efficiency, and enhancing the performance and interpretability of machine learning models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q9: \n",
    "\n",
    "i. What is the IQR? What criteria are used to assess it?\n",
    "\n",
    "ii. Describe the various components of a box plot in detail? When will the lower whisker surpass the upper whisker in length? How can box plots be used to identify outliers?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i. The IQR (Interquartile Range) is a statistical measure that provides information about the dispersion or spread of a dataset. It is calculated as the difference between the third quartile (Q3) and the first quartile (Q1). The IQR captures the range of the middle 50% of the data, excluding the highest and lowest 25%.\n",
    "\n",
    "To assess the IQR, the following criteria are commonly used:\n",
    "\n",
    "1. Outliers: Data points that fall below Q1 - 1.5 * IQR or above Q3 + 1.5 * IQR are considered outliers. These points are outside the \"whiskers\" of the box plot and are often depicted as individual data points beyond the whiskers or as individual dots.\n",
    "\n",
    "2. Skewness: The IQR, along with the median, can provide information about the skewness of the data distribution. If the IQR is symmetrically distributed around the median, it suggests a relatively symmetric dataset. However, if one end of the IQR is larger than the other, it indicates skewness in the data distribution.\n",
    "\n",
    "3. Variability: A larger IQR indicates greater variability or spread of the data, whereas a smaller IQR suggests less variability or a more concentrated dataset.\n",
    "\n",
    "ii. A box plot, also known as a box-and-whisker plot, provides a graphical representation of the distribution of a dataset. The various components of a box plot include:\n",
    "\n",
    "1. Median (Q2): The median represents the central value of the dataset. It divides the data into two equal halves, with 50% of the data points falling below and 50% above it.\n",
    "\n",
    "2. Box: The box in the plot represents the IQR, which encloses the middle 50% of the data. The lower boundary of the box is the first quartile (Q1), and the upper boundary is the third quartile (Q3). The height of the box represents the spread of the data within the middle 50% range.\n",
    "\n",
    "3. Whiskers: The whiskers extend from the box and represent the range of the data, excluding the outliers. They typically extend up to 1.5 times the IQR from the Q1 and Q3. Any data points beyond the whiskers are considered outliers.\n",
    "\n",
    "4. Lower Whisker: The lower whisker extends from the box to the smallest data point that is not considered an outlier.\n",
    "\n",
    "5. Upper Whisker: The upper whisker extends from the box to the largest data point that is not considered an outlier.\n",
    "\n",
    "The lower whisker will surpass the upper whisker in length when the data distribution is highly skewed to the left (negatively skewed) or has a long tail on the left side. In such cases, the lower whisker will extend further, indicating that the lower end of the data is more spread out than the upper end.\n",
    "\n",
    "Box plots can be used to identify outliers by examining data points beyond the whiskers. Any data point falling below Q1 - 1.5 * IQR or above Q3 + 1.5 * IQR is considered an outlier. Outliers are displayed individually as dots or data points beyond the whiskers in the box plot. Identifying outliers can help detect potential errors, anomalies, or extreme values that may impact the analysis or interpretation of the dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make brief notes on any two of the following:\n",
    "Data collected at regular intervals\n",
    "\n",
    "The gap between the quartiles\n",
    "\n",
    "Use a cross-tab\n",
    "\n",
    "The following are the breif notes about:\n",
    "\n",
    "1.Data collected at regular intervals:\n",
    "\n",
    "Interval data is one of the two types of discrete data.\n",
    "An example of interval data is the data collected on a thermometer—its gradation or markings are equidistant.\n",
    "Unlike ordinal data, interval data always take numerical values where the distance between two points on the\n",
    "scale is standardised and equal.\n",
    "2.The gap between the quartiles:\n",
    "\n",
    "Q1 is the first quartile of the data, i.e., to say 25% of the data lies between minimum and Q1.\n",
    "Q3 is the third quartile of the data, i.e., to say 75% of the data lies between minimum and Q3.\n",
    "The difference between Q3 and Q1 is called the Inter-Quartile Range or IQR.\n",
    "Make a comparison between:\n",
    "\n",
    "Data with nominal and ordinal values\n",
    "\n",
    "Histogram and box plot\n",
    "\n",
    "The average and median\n",
    "\n",
    "Ans: The following are the breif notes about:\n",
    "\n",
    "1.The average and median:\n",
    "\n",
    "The mean (informally, the “average“) is found by adding all of the numbers together and dividing by the number of items in the set: 10 + 10 + 20 + 40 + 70 / 5 = 30. The median is found by ordering the set from lowest to highest and finding the exact middle. The median is just the middle number: 20\n",
    "2.Histogram and barplot:\n",
    "\n",
    "Histograms and box plots are very similar in that they both help to visualize and describe numeric data. Although histograms are better in determining the underlying distribution of the data, box plots allow you to compare multiple data sets better than histograms as they are less detailed and take up less space."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
