{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: What is feature engineering, and how does it work? Explain the various aspects of feature\n",
    "engineering in depth.\n",
    "\n",
    "Feature engineering is the process of selecting,manipulated,and transformation raw data into feature that can be used in supervised learning.In order to make machine learning work well on new tasks,it might be neccessary to design and train better features.\n",
    "\n",
    "Feature engineering in ML consists of four main steps: Feature Creation,Transformations,Feature Extraction, and feature Selection.Feature engineering consists of creation,transformation,extraction, and selection of features, also known as variables, that are most conductive to createing an accurate ML algorithm."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2: What is feature selection, and how does it work? What is the aim of it? What are the various\n",
    "methods of function selection?\n",
    "\n",
    "Feature selection is the process where you automatically or manually select those features which contribute most to your prediction variable or output in which you are intreseted in. Having irrelevant features in your data can decrease the accuracy of the models and make your model learn based on irrelevent features.\n",
    "\n",
    "There are three types of feature selection:\n",
    "\n",
    "Wrapper methods(forward ,backward, and stepwise selection)\n",
    "\n",
    "Filter methods(ANOVA ,Pearson correlation ,variance threshoulding)\n",
    "\n",
    "Enbedded methods (Lasso, Ridge , Decision Tree)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3: Describe the function selection filter and wrapper approaches. State the pros and cons of each\n",
    "approach?\n",
    "\n",
    "The main differences between the filter and wrapper methods for feature selection are: Filter method measure the relevance of features by their correlation with dependent variable while wrapper methods measure the usefulness of a subset of feature by actually training a model on it.\n",
    "\n",
    "The filter method has the fastest running time however, it does not consider feature dependencies and trends to each feature seperately when uivariate techniques are used. The wrapper method has the advantages of better generalization and robust interaction with the classifier used for feature selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4: \n",
    "i. Describe the overall feature selection process.\n",
    "\n",
    "ii. Explain the key underlying principle of feature extraction using an example. What are the most widely used function extraction algorithms?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i. The overall feature selection process is a crucial step in machine learning and data analysis. It involves identifying and selecting the most relevant and informative features from the available dataset. The goal is to reduce the dimensionality of the data while retaining the most important information for the learning algorithm.\n",
    "\n",
    "The feature selection process typically consists of the following steps:\n",
    "\n",
    "1. Data Preparation: Preprocess the dataset by handling missing values, outliers, and performing any necessary data transformations or normalization.\n",
    "\n",
    "2. Feature Relevance Evaluation: Evaluate the relevance of each feature with respect to the target variable. This can be done using statistical measures like correlation coefficients, chi-square tests, or mutual information scores.\n",
    "\n",
    "3. Feature Ranking: Rank the features based on their relevance scores. This step helps in identifying the most important features that contribute significantly to the target variable.\n",
    "\n",
    "4. Feature Selection: Select a subset of the top-ranked features based on a certain criterion. There are various methods for feature selection, including filter methods, wrapper methods, and embedded methods.\n",
    "\n",
    "   - Filter Methods: These methods assess the relevance of features independently of the learning algorithm. Common filter methods include information gain, chi-square test, and correlation-based feature selection.\n",
    "   \n",
    "   - Wrapper Methods: These methods select features based on the performance of a specific learning algorithm. They involve searching through different subsets of features and evaluating the performance of the learning algorithm on each subset. Examples of wrapper methods are recursive feature elimination (RFE) and forward/backward selection.\n",
    "   \n",
    "   - Embedded Methods: These methods perform feature selection as part of the learning algorithm itself. They consider the feature selection process while training the model. Lasso regression and decision trees with feature importances are examples of embedded methods.\n",
    "\n",
    "5. Model Building and Evaluation: Build a predictive model using the selected features and evaluate its performance on a validation or test set. The selected features should lead to a model that generalizes well and performs effectively on unseen data.\n",
    "\n",
    "ii. The key underlying principle of feature extraction is to transform the original high-dimensional feature space into a lower-dimensional space, where the new features retain the most relevant information for the learning algorithm. This transformation is typically achieved by applying mathematical or statistical techniques.\n",
    "\n",
    "Let's consider an example of image recognition. Suppose we have a dataset of images, and each image is represented by a high-dimensional feature vector containing pixel intensities. However, using the raw pixel intensities as features for a machine learning algorithm may lead to high dimensionality and inefficiency. \n",
    "\n",
    "To address this, we can apply feature extraction techniques, such as Principal Component Analysis (PCA) or Convolutional Neural Networks (CNNs), to extract more compact and informative representations of the images.\n",
    "\n",
    "1. PCA: PCA is a dimensionality reduction technique that identifies the directions (principal components) along which the data varies the most. It transforms the high-dimensional image features into a lower-dimensional space while retaining the most significant information. These principal components are computed based on the covariance matrix of the original feature space. By selecting a subset of the top-ranked principal components, we can reduce the dimensionality of the image data while preserving the essential information for classification.\n",
    "\n",
    "2. CNNs: Convolutional Neural Networks are widely used for image recognition tasks. CNNs consist of multiple layers, including convolutional layers, pooling layers, and fully connected layers. The convolutional layers extract local patterns or features from the input images by applying convolution operations. The subsequent pooling layers reduce the spatial dimensions of the features while retaining their essential characteristics. Finally, the fully connected layers combine the extracted features and perform classification. In this case, the features extracted by the convolutional layers serve as the lower-dimensional representations of the original images.\n",
    "\n",
    "These are just a few examples of feature extraction algorithms commonly used in image recognition tasks. Other techniques such as Wavelet Transform, Histogram of Oriented Gradients (\n",
    "\n",
    "HOG), and deep learning architectures like autoencoders and recurrent neural networks can also be applied depending on the specific problem domain and data characteristics."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5: Describe the feature engineering process in the sense of a text categorization issue."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature engineering is a crucial step in text categorization, where the goal is to extract relevant and informative features from textual data to improve the performance of the classification model. The feature engineering process in text categorization involves several steps:\n",
    "\n",
    "1. Text Preprocessing: The first step is to preprocess the text data to clean and normalize it. This may include tasks such as removing punctuation, converting text to lowercase, removing stop words (commonly used words that do not carry much meaning), and applying stemming or lemmatization to reduce words to their base form.\n",
    "\n",
    "2. Tokenization: Tokenization is the process of splitting the text into individual words or tokens. It breaks down the text into meaningful units that can be further processed. This step helps in representing the text as a set of discrete features.\n",
    "\n",
    "3. Feature Extraction: Once the text is preprocessed and tokenized, features need to be extracted from the text data. Some commonly used feature extraction techniques include:\n",
    "\n",
    "   - Bag-of-Words (BoW): BoW represents the text by creating a vocabulary of all unique words in the corpus. Each document is then represented by a vector where each element represents the count or frequency of a word in the document. This approach disregards the order of the words but captures their occurrence.\n",
    "\n",
    "   - Term Frequency-Inverse Document Frequency (TF-IDF): TF-IDF is a statistical measure used to evaluate the importance of a word in a document relative to the entire corpus. It calculates a weight for each word based on its frequency in the document and its rarity across the corpus.\n",
    "\n",
    "   - Word Embeddings: Word embeddings, such as Word2Vec or GloVe, represent words as dense vectors in a continuous space, capturing semantic relationships between words. These pre-trained embeddings can be used as features or fine-tuned for the specific text categorization task.\n",
    "\n",
    "   - N-grams: N-grams represent contiguous sequences of n words. They capture the local context and can provide more information about the text. Common choices include unigrams (single words), bigrams (two-word sequences), or trigrams (three-word sequences).\n",
    "\n",
    "4. Feature Selection: After extracting the features, it is essential to select the most relevant and informative ones to reduce dimensionality and noise in the data. Feature selection techniques, such as information gain, chi-square test, or mutual information, can be applied to identify the features that have the highest correlation or impact on the target variable.\n",
    "\n",
    "5. Feature Engineering Techniques: In addition to traditional feature extraction, domain-specific knowledge can be utilized to engineer new features. For example, in text categorization, additional features can be created based on the presence of specific keywords, capitalization patterns, punctuation usage, or sentence structure. These engineered features can help capture specific characteristics or patterns in the text that are relevant to the categorization task.\n",
    "\n",
    "6. Model Building and Evaluation: Finally, the selected features are used to train a classification model, such as Naive Bayes, Support Vector Machines (SVM), or deep learning architectures like Recurrent Neural Networks (RNN) or Transformers. The model's performance is evaluated using appropriate metrics, such as accuracy, precision, recall, or F1-score, on a validation or test dataset.\n",
    "\n",
    "Iterative refinement of feature engineering, model selection, and hyperparameter tuning may be required to improve the performance of the text categorization system."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6: What makes cosine similarity a good metric for text categorization? A document-term matrix has\n",
    "two rows with values of (2, 3, 2, 0, 2, 3, 3, 0, 1) and (2, 1, 0, 0, 3, 2, 1, 3, 1). Find the resemblance in cosine.\n",
    "\n",
    "Cosine similarity is a metric used to measure how similar the documents are irresprectively os their size.The cosine similarity is advantageous because even if the two similar deocuments are far apart by the Eiclident distance (due to the size of the document), chances are they may still be orianted closer together.\n",
    "\n",
    "Cosine similarity is the cosine of the angle between two n-dimensional vectors in an n-dimentional space.It is the dot product of the two vectors divided by the product of the two vectors lengths (or magnitudes)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7: \n",
    "\n",
    "i. What is the formula for calculating Hamming distance? Between 10001011 and 11001111, calculate the Hamming gap.\n",
    "\n",
    "ii. Compare the Jaccard index and similarity matching coefficient of two features with values (1, 1, 0, 0, 1, 0, 1, 1) and (1, 1, 0, 0, 0, 1, 1, 1), respectively (1, 0, 0, 1, 1, 0, 0, 1)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i. The Hamming distance is a measure of the difference between two strings of equal length. It is calculated by counting the number of positions at which the corresponding elements of the two strings are different.\n",
    "\n",
    "The formula for calculating the Hamming distance is as follows:\n",
    "\n",
    "Hamming_distance = Sum(i=1 to n) (bit_i_XOR_bit_i')\n",
    "\n",
    "In this formula, n represents the length of the strings, bit_i represents the ith bit of the first string, and bit_i' represents the ith bit of the second string.\n",
    "\n",
    "Let's calculate the Hamming distance between the two binary strings: 10001011 and 11001111.\n",
    "\n",
    "10001011\n",
    "11001111\n",
    "---------\n",
    "^  ^  ^^ ^\n",
    "\n",
    "In this example, there are four positions where the corresponding bits differ (highlighted with ^). Therefore, the Hamming distance between the two strings is 4.\n",
    "\n",
    "ii. The Jaccard index and similarity matching coefficient are both measures of similarity between two sets. \n",
    "\n",
    "The Jaccard index is calculated as the ratio of the size of the intersection of two sets to the size of their union. The formula for the Jaccard index is as follows:\n",
    "\n",
    "Jaccard_index = |A ∩ B| / |A ∪ B|\n",
    "\n",
    "Where |A ∩ B| represents the size of the intersection of sets A and B, and |A ∪ B| represents the size of the union of sets A and B.\n",
    "\n",
    "Let's calculate the Jaccard index for the two sets with values (1, 1, 0, 0, 1, 0, 1, 1) and (1, 1, 0, 0, 0, 1, 1, 1):\n",
    "\n",
    "Set A = {1, 1, 0, 0, 1, 0, 1, 1}\n",
    "Set B = {1, 1, 0, 0, 0, 1, 1, 1}\n",
    "\n",
    "|A ∩ B| = 4 (elements 1, 2, 3, 4 are common)\n",
    "|A ∪ B| = 8 (total unique elements in both sets)\n",
    "\n",
    "Jaccard_index = |A ∩ B| / |A ∪ B| = 4 / 8 = 0.5\n",
    "\n",
    "The Jaccard index between the two sets is 0.5.\n",
    "\n",
    "The similarity matching coefficient, also known as the SMC, is calculated as the ratio of the number of matching attributes between two sets to the total number of attributes. The formula for the SMC is as follows:\n",
    "\n",
    "SMC = (a + d) / (a + b + c + d)\n",
    "\n",
    "Where a represents the number of matching attributes, b represents the number of attributes in set A but not in set B, c represents the number of attributes in set B but not in set A, and d represents the number of attributes that differ between set A and set B.\n",
    "\n",
    "Let's calculate the SMC for the two sets:\n",
    "\n",
    "Set A = (1, 1, 0, 0, 1, 0, 1, 1)\n",
    "Set B = (1, 0, 0, 1, 1, 0, 0, 1)\n",
    "\n",
    "a = 3 (attributes 1, 4, 5 match)\n",
    "b = 2 (attributes 2, 3 in set A)\n",
    "c = 2 (attributes 6, 7 in set B)\n",
    "d = 1 (attribute 8 differs)\n",
    "\n",
    "SMC = (a + d) / (a + b + c + d) = (3 + 1) / (3 + 2 +\n",
    "\n",
    " 2 + 1) = 4 / 8 = 0.5\n",
    "\n",
    "The similarity matching coefficient between the two sets is also 0.5."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8: State what is meant by 'high-dimensional data set'? Could you offer a few real-life examples?\n",
    "What are the difficulties in using machine learning techniques on a data set with many dimensions? What can be done about it?\n",
    "\n",
    "High dimension is when variable number p is higher than the sample sizes n i.e. p>n, cases. Higher dimensional data is referred to a data of n samples with p features, where p is larger than n.\n",
    "\n",
    "For example, tomographic imaging data, ECG data, and MEG data.One example of high dimentional data is microarray gene expression data.\n",
    "\n",
    "Make a few quick notes on:\n",
    "\n",
    "PCA is an acronym for Personal Computer Analysis.\n",
    "\n",
    "Use of vectors\n",
    "\n",
    "Embedded technique\n",
    "\n",
    "The Principal component analysis (PCA) is a technique used for identification of a smaller number of uncorrelated variables knowna as principal components from a larger set of data. The techique is widely used to emphasize variation and capture strong patterns in a data set.\n",
    "\n",
    "Vectors can be used to represent physical quantities.Most commonly in physics, vectors are used to represent displacement, velocity, and accelaration. Vectors are a combination of magnitude and direction, and are drawn as arrows In the context of machine learning , an embedding is a low-dimentional,learning continuous vector representation of discrete variables into which you can translate high-dimentional vectors.Generally, embeddings makes ML models more efficient and easier to work with, can be used with other models as well"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q9: Make a few quick notes on:\n",
    "\n",
    "PCA is an acronym for Personal Computer Analysis.\n",
    "\n",
    "Use of vectors\n",
    "\n",
    "Embedded technique"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. PCA (Principal Component Analysis):\n",
    "   - PCA stands for Principal Component Analysis, not Personal Computer Analysis.\n",
    "   - It is a dimensionality reduction technique used to transform high-dimensional data into a lower-dimensional space.\n",
    "   - PCA identifies the principal components, which are orthogonal directions that capture the maximum variance in the data.\n",
    "   - It helps in visualizing and understanding the underlying structure of the data by reducing its dimensionality.\n",
    "   - PCA is widely used in various fields such as image processing, genetics, finance, and natural language processing.\n",
    "\n",
    "2. Use of Vectors:\n",
    "   - Vectors are mathematical entities used to represent quantities that have both magnitude and direction.\n",
    "   - In machine learning and data analysis, vectors are commonly used to represent features or data points.\n",
    "   - Feature vectors are used to represent individual instances or samples in a dataset, where each element of the vector corresponds to a specific feature or attribute.\n",
    "   - Vectors can be operated on using various mathematical operations, such as addition, subtraction, dot product, or cross product.\n",
    "   - Vectors play a crucial role in linear algebra, which forms the foundation of many machine learning algorithms.\n",
    "\n",
    "3. Embedded Technique:\n",
    "   - The embedded technique refers to a feature selection or feature extraction method that is integrated into the learning algorithm itself.\n",
    "   - It considers the feature selection or extraction process as an integral part of the model training process.\n",
    "   - Embedded techniques aim to select or construct features that are most relevant and informative for the specific learning task.\n",
    "   - Examples of embedded techniques include Lasso regression, decision trees with feature importances, and deep learning architectures like convolutional neural networks (CNNs) and transformers.\n",
    "   - Embedded techniques can effectively capture complex relationships between features and the target variable, leading to improved model performance and interpretability."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q10: Make a comparison between:\n",
    "\n",
    "Sequential backward exclusion vs. sequential forward selection\n",
    "\n",
    "Function selection methods: filter vs. wrapper\n",
    "\n",
    "SMC vs. Jaccard coefficient\n",
    "\n",
    "Sequential floating forward selection (SFFS) starts from the empty set. After each forward step, SFFS performs backward steps as long as the objective function increases. Sequential floating backward selection (SFBS) starts from the full set.\n",
    "\n",
    "The jaccard coefficient is a measure of the percentage of overlap between sets defined as: (5.1) where W1 and W2 are two sets, in our case the 1-year windows of the ego networks.The jaccard coefficent can be a value between 0 and 1, with 0 indication no overlap and 1 complete overlap between the sets."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
