{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1:\n",
    "\n",
    "What is the estimated depth of a Decision Tree trained (unrestricted) on a one million instance\n",
    "training set?\n",
    "\n",
    "The estimated depth of a decision tree trained on a one million instance training set can vary widely depending on several factors such as the complexity of the problem, the number of features, the quality and distribution of the data, and the hyperparameters used for training.\n",
    "\n",
    "In general, an unrestricted decision tree trained on a large dataset tends to overfit and can become very deep, capturing all the noise in the data and reducing its ability to generalize to new, unseen data. However, some regularizing techniques such as pruning, limiting the depth or complexity of the tree, or using ensemble methods can help to prevent overfitting and improve the performance and interpretability of the model.\n",
    "\n",
    "As a rough estimate, a decision tree trained on a one million instance training set can have a depth of hundreds to thousands of nodes or levels, depending on the factors mentioned above. However, this is only a very rough estimate, and the actual depth of the tree can vary widely depending on the specific details of the problem and the data.\n",
    "\n",
    "Q2:\n",
    "\n",
    "Is the Gini impurity of a node usually lower or higher than that of its parent? Is it always\n",
    "lower/greater, or is it usually lower/greater?\n",
    "\n",
    "In a decision tree, the Gini impurity of a node is usually lower than or equal to that of its parent. This is because the split at each node is chosen to minimize the impurity of the resulting child nodes. The Gini impurity measures the degree of impurity or uncertainty of a node based on the class distribution of the data. A node with a lower Gini impurity indicates that it has a more homogeneous class distribution, which is desirable in a decision tree because it leads to better classification performance and a simpler, more interpretable tree.\n",
    "\n",
    "However, it is not always the case that the Gini impurity of a node is lower than or equal to that of its parent. In some cases, a split can result in child nodes with a higher Gini impurity than the parent, especially when the split is based on noisy or irrelevant features. In such cases, pruning or other regularization techniques may be used to avoid overfitting and improve the generalization performance of the tree.\n",
    "\n",
    "Q3.\n",
    "\n",
    " Explain if its a good idea to reduce max depth if a Decision Tree is overfitting the training set?\n",
    "\n",
    "Reducing the maximum depth of a decision tree can be an effective way to reduce overfitting of the training set. A decision tree with a large maximum depth can capture many details and nuances of the training data, including noise and outliers, which can lead to overfitting and poor generalization performance on new, unseen data. By reducing the maximum depth of the tree, we can limit its capacity to capture such details and promote more generalizable and interpretable models.\n",
    "\n",
    "Reducing the maximum depth can also help to improve the computational efficiency and scalability of the model, as fewer nodes and calculations are required to make predictions.\n",
    "\n",
    "However, reducing the maximum depth too much can also lead to underfitting, where the model becomes too simple and fails to capture the underlying patterns and structure of the data. Therefore, it is important to find the right balance between overfitting and underfitting by tuning the hyperparameters of the decision tree, such as the maximum depth, and evaluating the performance of the model on a validation set or through cross-validation.\n",
    "\n",
    "In summary, reducing the maximum depth of a decision tree can be a good idea if the model is overfitting the training set, but it should be done with care and in conjunction with other regularization techniques to prevent underfitting and improve generalization performance.\n",
    "\n",
    "Q4:\n",
    "\n",
    "Explain if its a good idea to try scaling the input features if a Decision Tree underfits the training\n",
    "set?\n",
    "\n",
    "Scaling the input features of a decision tree is unlikely to improve its performance if it is underfitting the training set. Unlike some other machine learning algorithms such as neural networks or support vector machines, decision trees are not sensitive to the scale or distribution of the input features. This is because the split points in a decision tree are based on the relative ordering of the features rather than their absolute values.\n",
    "\n",
    "If a decision tree is underfitting the training set, it is more likely due to a lack of complexity or expressiveness in the model rather than a scaling issue. In this case, increasing the complexity of the tree, for example by increasing the maximum depth or allowing more features per split, can help to capture more of the underlying patterns and structure of the data and improve the performance of the model.\n",
    "\n",
    "However, scaling the input features may still be beneficial in certain cases, such as when the features have vastly different scales or units, or when using some regularization techniques such as L1 or L2 regularization. In these cases, scaling the features can help to ensure that each feature is given equal weight in the model and prevent some features from dominating others.\n",
    "\n",
    "In summary, while scaling the input features of a decision tree is unlikely to improve its performance if it is underfitting the training set, it may still be beneficial in some cases for regularization or to ensure equal weighting of the features. However, increasing the complexity of the tree is generally a more effective approach to address underfitting.\n",
    "\n",
    "Q5:\n",
    "\n",
    "How much time will it take to train another Decision Tree on a training set of 10 million instances\n",
    "if it takes an hour to train a Decision Tree on a training set with 1 million instances?\n",
    "\n",
    "The time it takes to train a decision tree on a training set depends on several factors such as the complexity of the problem, the number of features, the quality and distribution of the data, and the hyperparameters used for training. Therefore, it is difficult to estimate the exact time it will take to train a decision tree on a training set of 10 million instances without additional information about the specific problem and the hardware and software used for training.\n",
    "\n",
    "However, as a rough estimate, we can assume that the time it takes to train a decision tree on a training set is roughly proportional to the size of the training set. Therefore, if it takes one hour to train a decision tree on a training set with 1 million instances, it may take approximately 10 hours to train another decision tree on a training set with 10 million instances, assuming that the problem and the hardware and software used for training are similar.\n",
    "\n",
    "It is worth noting that this is only a very rough estimate, and the actual time it takes to train the decision tree can vary widely depending on the specific details of the problem and the hardware and software used for training. Additionally, there may be other techniques such as parallelization or using a more efficient algorithm that can be used to speed up the training process.\n",
    "\n",
    "Q6:\n",
    "\n",
    "Will setting presort=True speed up training if your training set has 100,000 instances?\n",
    "\n",
    "Setting presort=True in the scikit-learn DecisionTreeClassifier or DecisionTreeRegressor can actually slow down the training process for a training set with 100,000 instances.\n",
    "\n",
    "The presort parameter is used to indicate whether the algorithm should presort the data before building the tree to speed up the computation of the best split. When presort=True, the algorithm will sort the data for each node before finding the best split. This can be beneficial for smaller datasets where the cost of sorting is relatively low compared to the cost of finding the best split. However, for larger datasets, the cost of sorting can become prohibitively high and may outweigh the benefits of faster split computation.\n",
    "\n",
    "For a training set with 100,000 instances, setting presort=True is unlikely to speed up the training process and may actually slow it down. This is because the overhead of sorting the data for each node can become very large for larger datasets. It is generally recommended to set presort=False for large datasets to avoid the overhead of sorting.\n",
    "\n",
    "In summary, setting presort=True is not recommended for training sets with 100,000 instances and may actually slow down the training process. It is generally more efficient to set presort=False for larger datasets.\n",
    "\n",
    "Q7.\n",
    "\n",
    " Follow these steps to train and fine-tune a Decision Tree for the moons dataset:\n",
    "\n",
    "a. To build a moons dataset, use make moons(n samples=10000, noise=0.4).\n",
    "\n",
    "b. Divide the dataset into a training and a test collection with train test split().\n",
    "\n",
    "c. To find good hyperparameters values for a DecisionTreeClassifier, use grid search with cross-\n",
    "validation (with the GridSearchCV class). Try different values for max leaf nodes.\n",
    "\n",
    "d. Use these hyperparameters to train the model on the entire training set, and then assess its\n",
    "output on the test set. You can achieve an accuracy of 85 to 87 percent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:  {'max_leaf_nodes': 10}\n",
      "Accuracy on the test set:  0.858\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Step 1: Build a moons dataset\n",
    "X, y = make_moons(n_samples=10000, noise=0.4)\n",
    "\n",
    "# Step 2: Divide the dataset into a training and a test collection\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Use grid search with cross-validation to find good hyperparameters values\n",
    "param_grid = {'max_leaf_nodes': [None, 10, 100, 500]}\n",
    "dtc = DecisionTreeClassifier()\n",
    "grid_search = GridSearchCV(dtc, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best hyperparameters: \", grid_search.best_params_)\n",
    "\n",
    "# Step 4: Train the model on the entire training set with the best hyperparameters\n",
    "best_dtc = grid_search.best_estimator_\n",
    "best_dtc.fit(X_train, y_train)\n",
    "\n",
    "# Step 5: Evaluate the model on the test set\n",
    "accuracy = best_dtc.score(X_test, y_test)\n",
    "print(\"Accuracy on the test set: \", accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8. \n",
    "\n",
    "Follow these steps to grow a forest:\n",
    "\n",
    "a. Using the same method as before, create 1,000 subsets of the training set, each containing\n",
    "100 instances chosen at random. You can do this with Scikit-ShuffleSplit Learn&#39;s class.\n",
    "\n",
    "b. Using the best hyperparameter values found in the previous exercise, train one Decision\n",
    "Tree on each subset. On the test collection, evaluate these 1,000 Decision Trees. These Decision\n",
    "\n",
    "Trees would likely perform worse than the first Decision Tree, achieving only around 80% accuracy,\n",
    "since they were trained on smaller sets.\n",
    "\n",
    "c. Now the magic begins. Create 1,000 Decision Tree predictions for each test set case, and\n",
    "keep only the most common prediction (you can do this with SciPy&#39;s mode() function). Over the test\n",
    "collection, this method gives you majority-vote predictions.\n",
    "\n",
    "d. On the test range, evaluate these predictions: you should achieve a slightly higher accuracy\n",
    "than the first model (approx 0.5 to 1.5 percent higher). You&#39;ve successfully learned a Random Forest\n",
    "classifier!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Create subsets of the training set: We will use Scikit-Learn's ShuffleSplit class to create 1,000 subsets, each containing 100 instances chosen randomly from the training set. Here's an example of how you can accomplish this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "# Create ShuffleSplit object\n",
    "shuffle_split = ShuffleSplit(n_splits=1000, train_size=100, random_state=42)\n",
    "\n",
    "# Generate subsets of the training set\n",
    "subsets = []\n",
    "for train_index, _ in shuffle_split.split(X_train):\n",
    "    subset = X_train[train_index]\n",
    "    subsets.append(subset)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Train Decision Trees on each subset: We will train one Decision Tree classifier on each of the 1,000 subsets using the best hyperparameter values found in the previous exercise. Here's an example of how you can train the Decision Trees:\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " c.Generate predictions using majority vote: We will generate 1,000 Decision Tree predictions for each instance in the test set and keep only the most common prediction for each instance. We can use SciPy's mode() function to accomplish this:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. Evaluate predictions on the test set: Finally, we can evaluate the majority-vote predictions on the test set to calculate the accuracy:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
