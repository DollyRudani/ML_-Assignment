{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1:\n",
    "\n",
    "What is the underlying concept of Support Vector Machines?\n",
    "\n",
    "The underlying concept of Support Vector Machines (SVMs) is to find the best decision boundary that separates two classes of data points with the largest possible margin. The margin is defined as the distance between the decision boundary and the closest data points from either class. SVMs seek to maximize this margin, as it is believed to lead to better generalization performance on unseen data.\n",
    "\n",
    "To find the best decision boundary, SVMs transform the input data into a higher-dimensional space using a kernel function, which allows for the creation of a hyperplane that can separate the classes. The hyperplane that separates the classes with the largest margin is known as the maximum margin hyperplane.\n",
    "\n",
    "However, not all datasets are linearly separable, and finding a hyperplane that perfectly separates the classes may not be possible. In such cases, SVMs allow for some misclassification errors and introduce a soft margin, which is a penalty for misclassifying data points. This soft margin allows for a more flexible decision boundary that can capture the underlying structure of the data while still seeking to maximize the margin.\n",
    "\n",
    "SVMs can also be extended to handle non-linearly separable datasets by using kernel functions that map the data into a higher-dimensional space, where it may be easier to find a separating hyperplane. Some commonly used kernel functions include linear, polynomial, radial basis function (RBF), and sigmoid kernels.\n",
    "\n",
    "Overall, SVMs aim to find a decision boundary that maximizes the margin between classes while allowing for some misclassification errors, making them a powerful tool for both linear and non-linear classification problems.\n",
    "\n",
    "Q2:\n",
    "\n",
    "What is the concept of a support vector?\n",
    "\n",
    "In Support Vector Machines (SVMs), a support vector is a data point that lies closest to the decision boundary or the maximum margin hyperplane. These data points play a crucial role in determining the decision boundary and the margin of the SVM classifier.\n",
    "\n",
    "The support vectors are the data points that are used to define the decision boundary or the maximum margin hyperplane. They are the data points that have a non-zero weight or importance in the SVM classifier. All other data points have a zero weight or importance and do not affect the decision boundary or the margin.\n",
    "\n",
    "The importance of support vectors lies in their ability to capture the underlying structure of the data and their influence on the SVM classifier's robustness and generalization performance. By focusing on the data points that are closest to the decision boundary, SVMs are able to generalize well to new and unseen data.\n",
    "\n",
    "Furthermore, support vectors also enable SVMs to handle non-linearly separable datasets by introducing a soft margin and allowing for some misclassification errors. In such cases, the support vectors play a crucial role in determining the shape of the decision boundary and the margin.\n",
    "\n",
    "Overall, support vectors are an essential concept in SVMs, as they determine the decision boundary and the margin of the classifier and help to ensure the classifier's robustness and generalization performance.\n",
    "\n",
    "Q3.:\n",
    "\n",
    "When using SVMs, why is it necessary to scale the inputs?\n",
    "\n",
    "When using Support Vector Machines (SVMs), it is generally necessary to scale the input features because SVMs are sensitive to the scale of the input data. The reason for this is that SVMs aim to find the maximum margin hyperplane that separates the classes of data points. The margin is defined as the distance between the decision boundary and the closest data points from either class. If the input features have different scales, then the distance metric used to define the margin may be dominated by features with larger scales, making it difficult for SVMs to find the optimal decision boundary.\n",
    "\n",
    "For example, consider a dataset with two features, one of which has values in the range of 0 to 1 and the other in the range of 0 to 1000. The feature with larger values will dominate the distance metric used to define the margin, and the decision boundary may be skewed towards this feature, resulting in poor performance on unseen data.\n",
    "\n",
    "Scaling the input features helps to overcome this problem by ensuring that each feature has a similar scale, allowing SVMs to make more accurate and robust predictions. Additionally, scaling can improve the convergence speed of the SVM algorithm and reduce the training time.\n",
    "\n",
    "There are several scaling methods that can be used, such as standardization, normalization, and min-max scaling, among others. The choice of scaling method will depend on the specific characteristics of the data and the requirements of the problem at hand.\n",
    "\n",
    "Q4:\n",
    "\n",
    "When an SVM classifier classifies a case, can it output a confidence score? What about a\n",
    "percentage chance?\n",
    "\n",
    "Yes, an SVM classifier can output a confidence score, which is often referred to as a decision function score or a distance from the decision boundary. The decision function score represents the distance from the decision boundary, and a higher score indicates higher confidence in the predicted class.\n",
    "\n",
    "However, the decision function score from an SVM classifier does not represent a percentage chance or a probability estimate, like some other classifiers such as logistic regression or Naive Bayes. The decision function score is a continuous value that represents the confidence of the SVM classifier in the predicted class, but it does not represent the probability or likelihood of the predicted class.\n",
    "\n",
    "To obtain a probability estimate from an SVM classifier, one can use a calibration method, such as Platt scaling or isotonic regression. These methods can be used to transform the decision function score into a probability estimate, allowing for a more intuitive interpretation of the classifier's output. However, it's important to note that the probability estimates obtained from calibration methods may not be as accurate as those obtained from classifiers that are specifically designed to provide probability estimates, such as logistic regression.\n",
    "\n",
    "Q5:\n",
    "\n",
    " Should you train a model on a training set with millions of instances and hundreds of features\n",
    "using the primal or dual form of the SVM problem?\n",
    "\n",
    "The choice of using the primal or dual form of the SVM problem depends on the specific characteristics of the data and the requirements of the problem at hand. However, in general, if you have a training set with millions of instances and hundreds of features, it is recommended to use the dual form of the SVM problem.\n",
    "\n",
    "The reason for this is that the primal form of the SVM problem has a computational complexity of O(m x n), where m is the number of instances and n is the number of features. This means that as the number of instances and features increases, the computational time required to solve the primal problem also increases significantly, making it infeasible for large datasets.\n",
    "\n",
    "On the other hand, the dual form of the SVM problem has a computational complexity of O(m^2 x n) or O(m^3), depending on the kernel function used. While the dual form has a higher computational complexity than the primal form, it is more computationally efficient for large datasets because it involves computing the dot products between all pairs of instances only once and then solving a smaller optimization problem involving the dual coefficients.\n",
    "\n",
    "Therefore, if you have a training set with millions of instances and hundreds of features, it is generally recommended to use the dual form of the SVM problem. However, it's important to note that the choice of kernel function can also impact the computational complexity and efficiency of the SVM algorithm, so it's important to consider the specific characteristics of the data and the requirements of the problem when selecting a kernel function.\n",
    "\n",
    "Q6:\n",
    "\n",
    "Let's say you've used an RBF kernel to train an SVM classifier, but it appears to underfit the\n",
    "training collection. Is it better to raise or lower (gamma)? What about the letter C?\n",
    "\n",
    "If an SVM classifier trained with an RBF kernel appears to underfit the training data, it may be necessary to adjust the hyperparameters gamma and C to improve the performance of the model.\n",
    "\n",
    "Gamma controls the shape of the decision boundary and the flexibility of the model. A smaller gamma value results in a wider decision boundary and a more flexible model, which can better fit the training data but may lead to overfitting. On the other hand, a larger gamma value results in a narrower decision boundary and a less flexible model, which can better generalize to new data but may result in underfitting.\n",
    "\n",
    "Therefore, if the RBF kernel SVM classifier appears to underfit the training data, it may be better to increase the gamma value to make the model more flexible and better fit the training data.\n",
    "\n",
    "C controls the trade-off between maximizing the margin and minimizing the classification error. A smaller C value puts more emphasis on maximizing the margin and may result in a wider margin, but may lead to underfitting. On the other hand, a larger C value puts more emphasis on minimizing the classification error and may result in a narrower margin, but may lead to overfitting.\n",
    "\n",
    "Therefore, if the RBF kernel SVM classifier appears to underfit the training data, it may also be necessary to increase the C value to allow the model to be more flexible and make more classification errors to better fit the training data.\n",
    "\n",
    "It's important to note that the optimal values of gamma and C may depend on the specific characteristics of the data and the requirements of the problem. It's often a good practice to perform a grid search or use cross-validation to determine the optimal hyperparameters for the model.\n",
    "\n",
    "Q7:\n",
    "\n",
    "To solve the soft margin linear SVM classifier problem with an off-the-shelf QP solver, how should\n",
    "the QP parameters (H, f, A, and b) be set?\n",
    "\n",
    "To solve the soft margin linear SVM classifier problem with an off-the-shelf QP solver, the QP parameters (H, f, A, and b) should be set as follows:\n",
    "\n",
    "H: The H matrix is a matrix of size (m + 1) x (m + 1), where m is the number of training instances. The first m rows and columns are filled with the identity matrix multiplied by the regularization parameter C. The last row and column are set to 0, as there is no penalty for the bias term. The H matrix ensures that the SVM finds a solution with a large margin while minimizing the classification error.\n",
    "\n",
    "f: The f vector is a vector of size (m + 1) x 1. The first m elements are set to -1, as we are trying to minimize the cost function. The last element is set to 0, as there is no penalty for the bias term.\n",
    "\n",
    "A: The A matrix is a matrix of size (m x (m + 1)). The first m columns are filled with the transpose of the matrix containing the product of the training instances with their corresponding labels (i.e., X times y). The last column is filled with -1 times the bias term. The A matrix ensures that the SVM finds a hyperplane that separates the two classes.\n",
    "\n",
    "b: The b vector is a vector of size (m x 1). It is filled with -1 to ensure that all the training instances are classified correctly.\n",
    "\n",
    "Once these parameters are set, they can be passed to the off-the-shelf QP solver, which will find the values of the dual coefficients that maximize the dual problem, and therefore, the values of the primal coefficients that minimize the cost function. These coefficients can then be used to make predictions on new data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8:\n",
    "\n",
    "On a linearly separable dataset, train a LinearSVC. Then, using the same dataset, train an SVC and\n",
    "an SGDClassifier. See if you can get them to make a model that is similar to yours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of LinearSVC: 1.0\n",
      "Accuracy of SVC: 1.0\n",
      "Accuracy of SGDClassifier: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate a synthetic dataset\n",
    "X, y = make_blobs(n_samples=1000, centers=2, random_state=42)\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train the models\n",
    "linear_svc = LinearSVC(loss='hinge', random_state=42)\n",
    "svc = SVC(kernel='linear', C=1, random_state=42)\n",
    "sgd = SGDClassifier(loss='hinge', learning_rate='constant', eta0=0.001, alpha=0.1, random_state=42)\n",
    "linear_svc.fit(X_train, y_train)\n",
    "svc.fit(X_train, y_train)\n",
    "sgd.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the models\n",
    "y_pred_linear_svc = linear_svc.predict(X_test)\n",
    "accuracy_linear_svc = accuracy_score(y_test, y_pred_linear_svc)\n",
    "\n",
    "y_pred_svc = svc.predict(X_test)\n",
    "accuracy_svc = accuracy_score(y_test, y_pred_svc)\n",
    "\n",
    "y_pred_sgd = sgd.predict(X_test)\n",
    "accuracy_sgd = accuracy_score(y_test, y_pred_sgd)\n",
    "\n",
    "print('Accuracy of LinearSVC:', accuracy_linear_svc)\n",
    "print('Accuracy of SVC:', accuracy_svc)\n",
    "print('Accuracy of SGDClassifier:', accuracy_sgd)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q9:\n",
    "\n",
    "On the MNIST dataset, train an SVM classifier. You'll need to use one-versus-the-rest to assign all\n",
    "10 digits because SVM classifiers are binary classifiers. To accelerate up the process, you might want to tune the hyperparameters using small validation sets. What level of precision can you achieve?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dolly\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\datasets\\_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "try:\n",
    "    # Load the dataset\n",
    "    mnist = fetch_openml('mnist_784')\n",
    "except KeyboardInterrupt:\n",
    "    # Retry the fetch_openml function if interrupted\n",
    "    mnist = fetch_openml('mnist_784')\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(mnist.data, mnist.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train the model\n",
    "svm_clf = SVC(kernel='rbf', decision_function_shape='ovr')\n",
    "\n",
    "# Tune the hyperparameters\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'gamma': [0.01, 0.1, 1],\n",
    "}\n",
    "grid_search = GridSearchCV(svm_clf, param_grid, cv=3)\n",
    "grid_search.fit(X_train[:10000], y_train[:10000])\n",
    "print('Best parameters:', grid_search.best_params_)\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "svm_clf = SVC(kernel='rbf', decision_function_shape='ovr', C=grid_search.best_params_['C'], gamma=grid_search.best_params_['gamma'])\n",
    "svm_clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = svm_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print('Accuracy:', accuracy)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q10:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.5779714155407936\n",
      "R-squared score: 0.5589381443977395\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load the dataset\n",
    "california_housing = fetch_california_housing()\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(california_housing.data, california_housing.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train the model\n",
    "svm_reg = SVR(kernel='linear', C=1.0, epsilon=0.2)\n",
    "svm_reg.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = svm_reg.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print('MSE:', mse)\n",
    "print('R-squared score:', r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
