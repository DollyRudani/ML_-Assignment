{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: What exactly is a feature? Give an example to illustrate your point.\n",
    "\n",
    "Features are the basic building blocks of datasets.The quality of the features in your dataset has a major impact on the quality of the insights you will gain when you use that dataset of machine learning.\n",
    "\n",
    "Additionally, different bussiness problems within the same industry do not neccessarily require the same features, Which is why it is important to have a strong understanding of the business goals of your data science project."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2: What are the various circumstances in which feature construction is required?\n",
    "\n",
    "The feature in your data will directly influence the predictive models you use and the results you can achieve. your results are dependent on many inter- dependent properties.You need great features that describe the structures inherent in your data. Better features means flexiblility."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3: Describe how nominal variables are encoded.\n",
    "\n",
    "Nomial data is made of discrete values with no numerical relationship between the different categories - mean and median are meaningless.Animal species is one example.For example, pig is not higher than bird and lower than fish."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4: Describe how numeric features are converted to categorical features.\n",
    "\n",
    "Converting categorical features into numeric features using domain knowledge.For example, we are given a list of countries and say we know the distance to these countries from india then we can replace it with distance from India. So, every country can be represented as its distance from india."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5: Describe the feature selection wrapper approach. State the advantages and disadvantages of this\n",
    "approach?\n",
    "\n",
    "Wrapper method measure the \"usefulness\" of features based on the classifier performance.In contrast,the filter methods pick up the intrinstic properties of the features (i.e., the \"relevance\" of the features) measured via univariate statistics instead of cross-validation performance.\n",
    "\n",
    "The wrapper classification algorithum with joint dimensionality reduction and classification can also be used but these method have high computaion cost,lowe discririnatinative power. Moreover,these methods depend on the efficient selection of classifiers for obtaining high accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6: When is a feature considered irrelevant? What can be said to quantify it?\n",
    "\n",
    "Features are considered relevant if they are either strongly or weakly relevant, and are considered irrelevant\n",
    "otherwise.\n",
    "\n",
    "Irrelevant features can never cotributed to prediction accuracy , by definition.Also to quantify it will need to first check the list of features, There are three types of feature selection:\n",
    "\n",
    "Wrapper method (forward, backward, and stepwise selection)\n",
    "\n",
    "Filter methods (ANOVA, Pearson correlation, variance threshoulding)\n",
    "\n",
    "Embedded methods (Lasso, Ridge,Decision Tree)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7: When is a function considered redundant? What criteria are used to identify features that could\n",
    "be redundant?\n",
    "\n",
    "If two features {X1,X2} are highly correlated, then the two features become redundant features since they have same information in terms of correlation measure. In other words, the correlation measure provides statistical association between any given a pair of features.\n",
    "\n",
    "Minimum redundancy feature selection is an algorithum freqently used in a method to accuratelty identify characteristics of genes and phenotypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8: What are the various distance measurements used to determine feature similarity?\n",
    "\n",
    "Four of the most commonly used distance measures in machine learning are as follows:\n",
    "\n",
    "Hamming Distance \n",
    "\n",
    "Euclidean Distance \n",
    "\n",
    "Manhattan Distance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q9: State difference between Euclidean and Manhattan distances?\n",
    "\n",
    "Euclidean & Hamming distances are used to measure similarity or dissimilarity between two sequences.Euclidean distance is extensively applied in analysis of convolutaional codes and Trellis codes.\n",
    "\n",
    "Hamming distance is frequently encountered in the analysis of block codes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q10: Distinguish between feature transformation and feature selection.\n",
    "\n",
    "Feature selection is for filtering irrelevant or redundant features from your dataset. The key difference between feature selection and extraction is that feature selection keeps a subset of the original features while feature extraction creates brand new ones."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q11: Make brief notes on any two of the following:\n",
    "\n",
    "1.SVD (Standard Variable Diameter Diameter)\n",
    "\n",
    "2. Collection of features using a hybrid approach\n",
    "\n",
    "3. The width of the silhouette\n",
    "\n",
    "4. Receiver operating characteristic curve"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. SVD (Singular Value Decomposition):\n",
    "- SVD is a matrix factorization technique used in linear algebra and numerical analysis.\n",
    "- It decomposes a matrix into three separate matrices: U, Σ, and V.\n",
    "- U represents the left singular vectors, Σ contains the singular values on its diagonal, and V represents the right singular vectors.\n",
    "- SVD has various applications, including data compression, image processing, recommendation systems, and dimensionality reduction.\n",
    "- It is commonly used in machine learning algorithms such as principal component analysis (PCA) and collaborative filtering.\n",
    "\n",
    "3. The width of the silhouette:\n",
    "- The width of the silhouette is a measure used in clustering analysis to assess the quality and separation of clusters.\n",
    "- It quantifies how well an object fits into its assigned cluster compared to other clusters.\n",
    "- It is calculated as the difference between the average dissimilarity of an object with objects in its own cluster (intra-cluster distance) and the average dissimilarity with objects in the nearest neighboring cluster (inter-cluster distance).\n",
    "- The width of the silhouette ranges from -1 to 1, where a higher value indicates better clustering.\n",
    "- A value close to 1 suggests that the object is well-matched to its cluster, while a value close to -1 indicates that the object might belong to the wrong cluster.\n",
    "\n",
    "(Note: The term \"Standard Variable Diameter Diameter\" (SVD) mentioned in option 1 doesn't appear to have a widely recognized meaning or relevance in the given context. It is possible that it could be a specific term or concept related to a particular domain or industry that I am not familiar with.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
